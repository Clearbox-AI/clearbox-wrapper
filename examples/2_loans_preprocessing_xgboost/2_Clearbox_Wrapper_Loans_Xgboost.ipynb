{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clearbox Wrapper Tutorial\n",
    "\n",
    "Clearbox Wrapper is a Python library to package and save a ML model.\n",
    "\n",
    "We'll use the Lending Club Loans dataset and build a Xgboost classifier on it.\n",
    "\n",
    "Before feeding the data to the model, we need to preprocess them. Preprocessing code is usually written as a separate element wrt to the model, during the development phase. We want to wrap and save the pre-processing along with the model so to have a pipeline Processing+Model ready to take unprocessed data, process them and make predictions.\n",
    "\n",
    "We can do that with Clearbox Wrapper, but all the preprocessing code must be wrapped in a single function. In this way, we can pass the function to the _save_model_ method.\n",
    "\n",
    "## Install and import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install scikit-learn\n",
    "!pip install xgboost\n",
    "\n",
    "!pip install clearbox-wrapper==0.3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import clearbox_wrapper as cbw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "We have two different csv files for the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_training_csv_path = 'loans_training_set.csv'\n",
    "loans_test_csv_path = 'loans_test_set.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_training = pd.read_csv(loans_training_csv_path)\n",
    "loans_test = pd.read_csv(loans_test_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'loan_risk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = loans_training[target_column]\n",
    "X_train = loans_training.drop(target_column, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = loans_test[target_column]\n",
    "X_test = loans_test.drop(target_column, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 31148 entries, 0 to 31147\nData columns (total 19 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   loan_amount         31148 non-null  int64  \n 1   payments_term       31148 non-null  object \n 2   monthly_payment     31148 non-null  float64\n 3   grade               31148 non-null  int64  \n 4   working_years       31148 non-null  int64  \n 5   home                31148 non-null  object \n 6   annual_income       31148 non-null  float64\n 7   verification        31148 non-null  object \n 8   purpose             31148 non-null  object \n 9   debt_to_income      31148 non-null  float64\n 10  delinquency         31148 non-null  int64  \n 11  inquiries           31148 non-null  int64  \n 12  open_credit_lines   31148 non-null  int64  \n 13  derogatory_records  31148 non-null  int64  \n 14  revolving_balance   31148 non-null  int64  \n 15  revolving_rate      31148 non-null  float64\n 16  total_accounts      31148 non-null  int64  \n 17  bankruptcies        31148 non-null  int64  \n 18  fico_average        31148 non-null  int64  \ndtypes: float64(4), int64(11), object(4)\nmemory usage: 4.5+ MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7788 entries, 0 to 7787\nData columns (total 19 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   loan_amount         7788 non-null   int64  \n 1   payments_term       7788 non-null   object \n 2   monthly_payment     7788 non-null   float64\n 3   grade               7788 non-null   int64  \n 4   working_years       7788 non-null   int64  \n 5   home                7788 non-null   object \n 6   annual_income       7788 non-null   float64\n 7   verification        7788 non-null   object \n 8   purpose             7788 non-null   object \n 9   debt_to_income      7788 non-null   float64\n 10  delinquency         7788 non-null   int64  \n 11  inquiries           7788 non-null   int64  \n 12  open_credit_lines   7788 non-null   int64  \n 13  derogatory_records  7788 non-null   int64  \n 14  revolving_balance   7788 non-null   int64  \n 15  revolving_rate      7788 non-null   float64\n 16  total_accounts      7788 non-null   int64  \n 17  bankruptcies        7788 non-null   int64  \n 18  fico_average        7788 non-null   int64  \ndtypes: float64(4), int64(11), object(4)\nmemory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a preprocessing function\n",
    "\n",
    "The data need to be preprocessed before be passed as input to the model. You can use your own custom code for the preprocessing, just remember to wrap all of it in a single function.\n",
    "\n",
    "The following preprocessing is built using several functions offered by Scikit Learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ordinal_features = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "\n",
    "ordinal_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
    "                                      ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                                          ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "x_preprocessor = ColumnTransformer(transformers=[('num', ordinal_transformer, ordinal_features),\n",
    "                                               ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('num',\n",
       "                                 Pipeline(steps=[('imputer',\n",
       "                                                  SimpleImputer(strategy='median')),\n",
       "                                                 ('scaler', StandardScaler())]),\n",
       "                                 ['loan_amount', 'monthly_payment', 'grade',\n",
       "                                  'working_years', 'annual_income',\n",
       "                                  'debt_to_income', 'delinquency', 'inquiries',\n",
       "                                  'open_credit_lines', 'derogatory_records',\n",
       "                                  'revolving_balance', 'revolving_rate',\n",
       "                                  'total_accounts', 'bankruptcies',\n",
       "                                  'fico_average']),\n",
       "                                ('cat',\n",
       "                                 Pipeline(steps=[('imputer',\n",
       "                                                  SimpleImputer(strategy='most_frequent')),\n",
       "                                                 ('onehot',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                 ['payments_term', 'home', 'verification',\n",
       "                                  'purpose'])])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "x_preprocessor.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual we encode the Y labels through a simple LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_encoding = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "y_encoding.fit(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train the model\n",
    "\n",
    "We build a simple Xgboost classifier setting up some basic parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier(n_estimators=30, max_depth=8, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed to pre-process the X and encode the Y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = x_preprocessor.transform(X_train)\n",
    "X_test_processed = x_preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoded = y_encoding.transform(y_train)\n",
    "y_test_encoded = y_encoding.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we fit the model on the processed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/andrea/clearbox_repos/clearbox-wrapper/.venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "[08:47:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=8,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=30, n_jobs=8, num_parallel_tree=1, random_state=42,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "xgb_clf.fit(X_train_processed, y_train_encoded)"
   ]
  },
  {
   "source": [
    "We show some metrics on the training set..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_predictions = xgb_clf.predict(X_train_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Metrics Report:\n               precision    recall  f1-score   support\n\n           0       0.98      0.35      0.51      5433\n           1       0.88      1.00      0.93     25715\n\n    accuracy                           0.88     31148\n   macro avg       0.93      0.67      0.72     31148\nweighted avg       0.90      0.88      0.86     31148\n\n"
     ]
    }
   ],
   "source": [
    "print('Training Metrics Report:\\n', metrics.classification_report(y_train_encoded, training_predictions))"
   ]
  },
  {
   "source": [
    "...and on the test set:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = xgb_clf.predict(X_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Metrics Report:\n               precision    recall  f1-score   support\n\n           0       0.82      0.22      0.35      1364\n           1       0.86      0.99      0.92      6424\n\n    accuracy                           0.85      7788\n   macro avg       0.84      0.60      0.63      7788\nweighted avg       0.85      0.85      0.82      7788\n\n"
     ]
    }
   ],
   "source": [
    "print('Test Metrics Report:\\n', metrics.classification_report(y_test_encoded, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap and Save the Model\n",
    "\n",
    "Finally, we use Clearbox Wrapper to wrap and save the model and the preprocessor as a zipped folder in a specified path. \n",
    "\n",
    "The model dependency (xgboost) and its version it is detected automatically by CBW and added to the requirements saved into the resulting folder. But (**IMPORTANT**) you need to pass as a parameter the additional dependencies required for the preprocessing as a list. We need to add Scikit-Learn in this case.\n",
    "\n",
    "We pass the training dataset to `save_model` as well, in order to generate a Preprocessing and Model Signature (the signature represents input and (optionally) outputs as data frames with (optionally) named columns and data type).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_model_path = 'loans_xgboost_wrapped_model_v0.3.10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_dependencies = [\"scikit-learn==0.23.2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbw.save_model(wrapped_model_path, xgb_clf, preprocessing=x_preprocessor, input_data=X_train, additional_deps=processing_dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip and load the model\n",
    "\n",
    "The following cells are not necessary for the final users, the zip created should be uploaded to our SAAS as it is. But here we want to show how to load a saved model and compare it to the original one. Some lines similar to these are present in the backend of Clearbox AI SAAS.\n",
    "\n",
    "**IMPORTANT**: To assure reproducibility and avoid loading errors, it is necessary to load the wrapped model with the same Python version with which the model was saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_model_path = 'loans_xgboost_wrapped_model_v0.3.10.zip'\n",
    "unzipped_model_path = 'loans_xgboost_wrapped_model_v0.3.10_unzipped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(zipped_model_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(unzipped_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = cbw.load_model(unzipped_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the original model, the input data (X_test) must goes through the preprocessing function before the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_predictions = xgb_clf.predict(X_test_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the wrapped model, **the preprocessing is part of the predict pipeline**, so we can pass directly the raw input data to the predict function of the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loaded_model_predictions = loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check that the predictions made with the original model and the wrapped one are equal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_array_equal(original_model_predictions, loaded_model_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove all generated files and directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(zipped_model_path):\n",
    "        os.remove(zipped_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(unzipped_model_path):\n",
    "        shutil.rmtree(unzipped_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python38564bitvenvvenv859ca4a018fa42de98d84719ac267976"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
