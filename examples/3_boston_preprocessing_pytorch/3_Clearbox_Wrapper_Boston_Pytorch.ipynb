{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clearbox Wrapper Tutorial\n",
    "\n",
    "Clearbox Wrapper is a Python library to package and save a ML model.\n",
    "\n",
    "We'll use the popular Boston Housing dataset and build a Pytorch regressor on it.\n",
    "\n",
    "This is a typical case: before feeding the data to the model, we need to pre-process (scaling) them. Preprocessing code is usually written as a separate element wrt to the model, during the development phase. We want to wrap and save the pre-processing along with the model so to have a pipeline Processing+Model ready to take unprocessed data, process them and make predictions.\n",
    "\n",
    "We can do that with Clearbox Wrapper, but all the preprocessing code must be wrapped in a single function. In this way, we can pass the function to the _save_model_ method.\n",
    "\n",
    "## Install and import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install scikit-learn\n",
    "!pip install torch\n",
    "\n",
    "!pip install clearbox-wrapper==0.3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import clearbox_wrapper as cbw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "We have two different csv files for the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_training_csv_path = 'boston_training_set.csv'\n",
    "boston_test_csv_path = 'boston_test_set.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_training = pd.read_csv(boston_training_csv_path)\n",
    "boston_test = pd.read_csv(boston_test_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'MEDV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = boston_training[target_column]\n",
    "X_train = boston_training.drop(target_column, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = boston_test[target_column]\n",
    "X_test = boston_test.drop(target_column, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 404 entries, 0 to 403\nData columns (total 13 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   CRIM     404 non-null    float64\n 1   ZN       404 non-null    float64\n 2   INDUS    404 non-null    float64\n 3   CHAS     404 non-null    int64  \n 4   NOX      404 non-null    float64\n 5   RM       404 non-null    float64\n 6   AGE      404 non-null    float64\n 7   DIS      404 non-null    float64\n 8   RAD      404 non-null    int64  \n 9   TAX      404 non-null    int64  \n 10  PTRATIO  404 non-null    float64\n 11  B        404 non-null    float64\n 12  LSTAT    404 non-null    float64\ndtypes: float64(10), int64(3)\nmemory usage: 41.2 KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 102 entries, 0 to 101\nData columns (total 13 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   CRIM     102 non-null    float64\n 1   ZN       102 non-null    float64\n 2   INDUS    102 non-null    float64\n 3   CHAS     102 non-null    int64  \n 4   NOX      102 non-null    float64\n 5   RM       102 non-null    float64\n 6   AGE      102 non-null    float64\n 7   DIS      102 non-null    float64\n 8   RAD      102 non-null    int64  \n 9   TAX      102 non-null    int64  \n 10  PTRATIO  102 non-null    float64\n 11  B        102 non-null    float64\n 12  LSTAT    102 non-null    float64\ndtypes: float64(10), int64(3)\nmemory usage: 10.5 KB\n"
     ]
    }
   ],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a preprocessing function\n",
    "\n",
    "The data need to be preprocessed before be passed as input to the model. You can use your own custom code for the preprocessing, just remember to wrap all of it in a single function.\n",
    "\n",
    "The following preprocessing makes no sense, it is provided just to show the possibilities offer by the wrapper.\n",
    "\n",
    "We fit a SKlearn scaler on the X training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RobustScaler()"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "robust_scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we wrap the processing into a function adding also some useless additional lines that increment by 1 all the values of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boston_preprocessing(x_data):\n",
    "    processed_data = robust_scaler.transform(x_data)\n",
    "    processed_data = processed_data + 1.0\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train the model\n",
    "\n",
    "We build a Pytorch network setting some basic parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BostonModel(nn.Module):\n",
    "    def __init__(self, n_features, hiddenA, hiddenB):\n",
    "        super(BostonModel, self).__init__()\n",
    "        self.linearA = nn.Linear(n_features, hiddenA)\n",
    "        self.linearB = nn.Linear(hiddenA, hiddenB)\n",
    "        self.linearC = nn.Linear(hiddenB, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        yA = F.relu(self.linearA(x))\n",
    "        yB = F.relu(self.linearB(yA))\n",
    "        return self.linearC(yB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...preprocess the training and test data through our function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = boston_preprocessing(X_train)\n",
    "X_train_processed_tensor = torch.Tensor(X_train_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_processed = boston_preprocessing(X_test)\n",
    "X_test_processed_tensor = torch.Tensor(X_test_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...convert the y training and test data to the Pytorch format as well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_torch = torch.Tensor(y_train.values)\n",
    "y_test_torch = torch.Tensor(y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and finally create a model instance and fit it on the resulting data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbc75f94c30>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = torch.utils.data.TensorDataset(X_train_processed_tensor, y_train_torch)\n",
    "train_loader = torch.utils.data.DataLoader(train_datasets, batch_size=25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_regr = BostonModel(X_train_processed_tensor.shape[1], 100, 50)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(torch_regr.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/andrea/clearbox_repos/clearbox-wrapper/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n",
      "Epoch #1\tLoss: 14388.408\n",
      "Epoch #26\tLoss: 4106.438\n",
      "Epoch #51\tLoss: 1636.331\n",
      "Epoch #76\tLoss: 1068.869\n",
      "Epoch #101\tLoss: 844.120\n",
      "Epoch #126\tLoss: 729.870\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 150\n",
    "all_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    losses = []\n",
    "    total = 0\n",
    "    for inputs, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = torch_regr(inputs)\n",
    "        loss = criterion(y_pred, torch.unsqueeze(target,dim=1))\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        total += 1\n",
    "\n",
    "    epoch_loss = sum(losses) / total\n",
    "    all_losses.append(epoch_loss)\n",
    "                \n",
    "    mess = f\"Epoch #{epoch+1}\\tLoss: {all_losses[-1]:.3f}\"\n",
    "    if (epoch%25 == 0):\n",
    "        print(mess)"
   ]
  },
  {
   "source": [
    "We show some metrics on the training set..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_regr.eval()\n",
    "training_predictions = torch_regr(X_train_processed_tensor).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Report\n\nTarget Column min: 5.0\nTarget Column max: 50.0\nTarget Column mean: 22.796534653465343\nTarget Column std: 9.332147158711562\n\nMax Error: 34.06868076324463\nMean Absolute Error: 3.6331796780671217\nMean Squared Error: 27.9573566724688\nR2 Score: 0.6781827873784485\n"
     ]
    }
   ],
   "source": [
    "print('Training Report\\n')\n",
    "print('Target Column min: {}'.format(y_train.min()))\n",
    "print('Target Column max: {}'.format(y_train.max()))\n",
    "print('Target Column mean: {}'.format(y_train.mean()))\n",
    "print('Target Column std: {}\\n'.format(y_train.std()))\n",
    "print(\"Max Error: {}\".format(metrics.max_error(y_train, training_predictions)))\n",
    "print(\"Mean Absolute Error: {}\".format(metrics.mean_absolute_error(y_train, training_predictions)))\n",
    "print(\"Mean Squared Error: {}\".format(metrics.mean_squared_error(y_train, training_predictions)))\n",
    "print(\"R2 Score: {}\".format(metrics.r2_score(y_train, training_predictions)))"
   ]
  },
  {
   "source": [
    "...and on the test set:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = torch_regr(X_test_processed_tensor).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Report\n\nTarget Column min: 5.0\nTarget Column max: 50.0\nTarget Column mean: 21.488235294117644\nTarget Column std: 8.60580386839697\n\nMax Error: 30.797901153564453\nMean Absolute Error: 3.4708297935186647\nMean Squared Error: 27.61264297859737\nR2 Score: 0.623466269042327\n"
     ]
    }
   ],
   "source": [
    "print('Test Report\\n')\n",
    "print('Target Column min: {}'.format(y_test.min()))\n",
    "print('Target Column max: {}'.format(y_test.max()))\n",
    "print('Target Column mean: {}'.format(y_test.mean()))\n",
    "print('Target Column std: {}\\n'.format(y_test.std()))\n",
    "print(\"Max Error: {}\".format(metrics.max_error(y_test, test_predictions)))\n",
    "print(\"Mean Absolute Error: {}\".format(metrics.mean_absolute_error(y_test, test_predictions)))\n",
    "print(\"Mean Squared Error: {}\".format(metrics.mean_squared_error(y_test, test_predictions)))\n",
    "print(\"R2 Score: {}\".format(metrics.r2_score(y_test, test_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap and Save the Model\n",
    "\n",
    "Finally, we use Clearbox Wrapper to wrap and save the model and the preprocessor as a zipped folder in a specified path. \n",
    "\n",
    "The model dependency (torch) and its version it is detected automatically by CBW and added to the requirements saved into the resulting folder. But (**IMPORTANT**) you need to pass as a parameter the additional dependencies required for the preprocessing as a list. We just need to add Scikit-Learn in this case.\n",
    "\n",
    "We pass the training dataset (X train) to `save_model` as well, in order to generate a Preprocessing and Model Signature (the signature represents input and (optionally) outputs as data frames with (optionally) named columns and data type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_model_path = 'boston_wrapped_model_v0.3.10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_dependencies = [\"scikit-learn==0.23.2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbw.save_model(wrapped_model_path, torch_regr, preprocessing=boston_preprocessing, input_data=X_train, additional_deps=processing_dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip and load the model\n",
    "\n",
    "The following cells are not necessary for the final users, the zip created should be uploaded to our SAAS as it is. But here we want to show how to load a saved model and compare it to the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_model_path = 'boston_wrapped_model_v0.3.10.zip'\n",
    "unzipped_model_path = 'boston_wrapped_model_v0.3.10_unzipped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(zipped_model_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(unzipped_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = cbw.load_model(unzipped_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_predictions = torch_regr(X_test_processed_tensor).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_predictions = loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_array_equal(original_model_predictions, loaded_model_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove all generated files and directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(zipped_model_path):\n",
    "        os.remove(zipped_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(unzipped_model_path):\n",
    "        shutil.rmtree(unzipped_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python38564bitvenvvenv859ca4a018fa42de98d84719ac267976"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
